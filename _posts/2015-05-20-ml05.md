---
layout: post
title: Machine Learning05:cost function&gradient descent&multi class
date:   2015-05-20
description: Machine Learning05:cost function&gradient descent&multi class
category: education
tags: [machinelearning,coursera]
---

### cost function:

用以下两个式子来做cost function

* cost function:-log(hθ(x)) 当y =1

* cost function:-log(1-hθ(x)) 当y = 0

可以看到当y为1时，如果hθ(x)得到的是1，那么cost function为0，如果hθ(x)得到的是0，那么cost function趋向于正无穷，当y为0时，如果hθ(x)得到的是1，那么cost function趋向于正无穷，得到的为0，cost function为0。

cost function是用来评价hypothesis对训练集的贴合度的，对于分类的情况，这样的cost function完美的表现了这一点，真是数学的魅力。

![09](http://obhvbhenx.bkt.clouddn.com//image/blog/coursera/ml_08_logistic_cost.png)

梯度函数
![09](http://obhvbhenx.bkt.clouddn.com//image/blog/coursera/ml_10_logistic_cost.png)

<!-- more -->

<pre>

J = sum( (-1 .* y) .* log( sigmoid(X * theta) ) - (1 - y) .* log ( 1-sigmoid( X*theta) )  )/m;
grad = ( X' * (sigmoid( X * theta ) - y ) )/ m

</pre>

### gradient descent

对复杂的函数gradient descent很难找到全局的最优解，只能找到局部的最优解，matlab和octave中提供了许多更复杂但普通人不容易理解的算法，我们只需要调用这些算法。

要调用这些算法我们必须向他提供cost function 和grad的计算方法。

需要用到的函数是matlab中的fminunc函数[wiki](http://cn.mathworks.com/help/optim/ug/fminunc.html?s_tid=srchtitle)

<pre>

options = optimset('GradObj', 'on', 'MaxIter', 400);
function [J, grad] = costFunction(theta, X, y)
/**/
[theta, cost] = ...
	fminunc(@(t)(costFunction(t, X, y)), initial_theta, options);
	
</pre>

### multiclass：one vs all

在对数据进行含有多个种类的分类时，如分为A，B，C，D四类，实际上是预测一个用例是不是属于A，是不是属于B，是不是属于C，是不是属于D的过程。

也就是对用例进行了四次分类，每次分类的结果都是是否属于某类，one vs all 就是建立多个分类器，对具有多个class的分类问题常采用这种方法。




