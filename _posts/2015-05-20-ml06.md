---
layout: post
title: Machine Learning06:regularization
description: 
category: blog
---

#####overfitting

当从训练集中提取过多的属性用来建立hypothesis时，可能导致过度适应数据，选取某些很复杂的方程可以很完美的贴合训练集数据，但是对要预测的用例可能并不会有很好的表现

为了解决这一点，实际上就要剔除掉模型中那些实际上导致overfitting的属性

![09](http://picturereq.herokuapp.com/images/coursera/ml_overfitting_source.png)

![09](http://picturereq.herokuapp.com/images/coursera/ml_overfitting.png)

如上图所示就是overfitting的情况，在做机器学习的时候，很多时候我们刚开始的时候并不知道选取哪些属性，只能尽可能多的选取属性来构建hypothesis拟合训练集中的数据，很容易导致overfitting的情况，但是我们又不能主观的减少选取的属性，为了解决overfitting问题必须提出一个新的cost function，而解决overfitting的过程称为regularization

#####new cost function

新的cost function除了原有的评价拟合度之外，又增加了一个新的项λ*sum(θ^2)，λ是regularization的系数，加入新的项之后，θ越大，cost function也就越大，在gradient descent中要减少cost function，也需要不断的减少θ，这样对拟合训练集中数据影响不大的feature对应的θ就会不断变小。

![09](http://picturereq.herokuapp.com/images/coursera/ml_reg_costfunction.png)

![09](http://picturereq.herokuapp.com/images/coursera/ml_reg_grad.png)

![09](http://picturereq.herokuapp.com/images/coursera/ml_reg_grad_θ0.png)

θ0不需要进行regularization

<pre>

theta_1=[0;theta(2:end)];
J = sum( (-1 .* y) .* log( sigmoid(X * theta) ) - (1 - y) .* log ( 1-sigmoid( X*theta) )  )/m + lambda/(2*m) * theta_1' * theta_1;
grad = ( X' * (sigmoid( X * theta ) - y ) )/ m + lambda/m * theta_1;

</pre>

#####λ的选取

lambda不能太大或太小，太小的话会减弱regularization的效果，太大的话会导致所有θ都很小，不能达到很好的拟合效果，反而会导致underfitting（即拟合效果不理想）

![09](http://picturereq.herokuapp.com/images/coursera/ml_overfitting_underfitting.png)

underfitting示例