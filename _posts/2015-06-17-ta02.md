---
layout: post
title: TextAnalytics
date:   2015-06-17
description: Coursera TextAnalytics
category: education
tags: [machinelearning,coursera]
---

### Text Mining and Analytics

paradigmatic关系，即同一类单词

同一类单词的关键是上下文相似性，评价相似性：单词出现在上下文中出现概率组成的向量相乘

减少the这样频繁出现单词的贡献度，log（the出现的文档数/总文档数）

<!-- more -->

synagmatic关系

两个单词同时出现的，即两个单词各自出现时对另一个的贡献度

熵，单词出现越容易发现规律，比如the，几乎一定会出现，熵越低

明白某个单词出现后对于预测另一个单词时会降低它的熵，让它更容易预测

mutual information相互关系，因为p（x|y)和p（x|z）不可比，所以用i(x,y)这样的形式，实际上转换成了单词对关联度的排名问题

### topic mining

输入:文档集合，主题集合

输出：主题集合

由每个文档关于某个主题的概率组成的向量

每个文档主题概率向量的和应该为1

### generative model for topic mining

topic representation：一个单词不能很好的表示一个主题，所以使用一个单词w或者词组的集合以及出现在这个主题中的概率(p w | θ)来表示一个主题，如下

text : 0.03
mining : 0.04
topic : 0.02
travel : 0.0000002
meat : 0.0000005

每个文档可能涵盖不同的主题，因此对主题θ1，θ2，θB...等主题分配一个权重p(θ)，对文本中每个单词都有P(w|θ1)*P(θ1)+P(w|θ2)*P(θ2)+....，由于P(θ)的和为1，最终要求的实际就是使所有单词得到的值的乘积最大的θ们。

θB：背景主题，类似the，a，there这样实际无意义的单词。

### bayesian & maximum likehood

最大似然估计考虑找出最好模型的拟合数据，贝叶斯同时考虑模型发生的可能性和对数据的拟合度，假设θ为模型参数，X为观测到的数据

maximum likehood：使p(X|θ)最大，即在所有模型中，最终得到的模型最有可能产生观测到的输出

bayesian：使P(X | θ) * P( θ )最大

### text clustering

使用topic mining 的方法可以对每个文本得到一个关于k个θ的覆盖率

不同之处