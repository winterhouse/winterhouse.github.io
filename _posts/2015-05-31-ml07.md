---
layout: post
title: Machine Learning07:Neural Networks
date:   2015-05-31
description: Machine Learning07:Neural Networks
category: education
tags: [machinelearning,coursera]
---

### why non-linear hypothesis

对logistics regression 和linear regression来说，选择一个足够复杂能够有很高预测精度的function就意味着要构建很多的feature，假如要构建一个二次方程形式的hθ，如果有100个attribute，就意味着要构建5000个feature，而且即使这样很多时候二次方程也不能很好的拟合数据。

在图像处理上，如果图像的像素很大，linear hypothesis要构建的feature就很多，运算量也会变的很大。

<!-- more -->

### background on neural networks

neural networks试图模仿大脑的工作原理，这里有一个one learning algorithm的概念，看了两遍才看明白，就是大脑对于视觉，触觉，听觉等等所有信息采用的都是一种处理方法，也就是说，如果把耳朵和负责听觉处理的大脑皮层切断，向负责听觉处理的大脑皮层传递视觉信号，负责听觉处理的大脑皮层同样可以处理，这样那部分大脑皮层就可以学会"看"。

基于这一点，有人将视觉信号传递到负责味觉，听觉等等的大脑皮层，用来设计帮助盲人建立视力的设备，如[brainport wiki](http://en.wikipedia.org/wiki/Brainport)和[haptic belt site](http://beagleboard.org/project/BlindLeadingBlind/)

如果能够找出并模仿大脑的这种one learning algorithm，那么我们也就有了一个可以解决一切问题的模型。

### representation&forward propagation

neurons[wiki](http://en.wikipedia.org/wiki/Neuron)，神经元有多个接受线和一个输出线，对于输入的[x1,x2,x3,...]，用权重[θ1,θ2,θ3,...]和sigmoid function hθ(x)计算可以得到一个输出。

许多接收同一组输入，并输出到同一个输出的neurons就组成了一层neurons，neural network[wiki](http://en.wikipedia.org/wiki/Artificial_neural_network)就是由许多层组成的neurons的网络，第一层是input layer，最后一层out layer，中间的层称为hidden layer。

![09](http://obhvbhenx.bkt.clouddn.com//image/blog/coursera/ml_neural_networks.png)

每一层的neurons都接收一组输入，并且输出一组经过每个neurons计算的输出。

对于每个neurons来说，实际上都是接收一组输入并且用一组θ和logistics function计算出一个输出，θ是属于neurons的，也就是每一个neurons都拥有自己的θ，输入经过一层hidden layer，就会得到一组输出作为新的输入，而这组新的输入又是根据那一层neurons自己的θ计算出来的，在训练的过程中，neurons会根据输入不断调整自己的θ，使整个neural networks的预测效果越来越好。

(PS:注意θ0，每一层在输入时都要重新加上θ0)