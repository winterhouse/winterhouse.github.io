---
layout: post
title: Machine Learning03:linear regression with Multiple Features
date:   2015-05-17
description: Machine Learning03:linear regression with Multiple Features
category: education
tags: [machinelearning,coursera]
---

### 1:Feature scaling

当含有多个feature的时候，每个feature的取值范围可能不同，但对于gradient descent算法来说，递减的幅度是由α来控制的。

如果有的feature取值范围是1~100,有的是1000~100000000000，在梯度递减时可能有的feature一步会迈的很大，而有的feature会迈的很小。

所以要对feature进行scaling，使所有feature的取值范围都大致处在-1<=x<=1中。

<!-- more -->

### 2:mean normalization

进行feature scaling的常用方法，对每一个feature值Xi，Xi-(middle(X))/(largest(X))，这样可以使每个feature的取值范围为-0.5<=x<=0.5

将数据标准化，可参考[wiki](http://en.wikipedia.org/wiki/Normalization_(statistics))

matlab示例

<pre>
 
meanvalue = mean(X);       %  mean value 
stdvalue = std(X);     %  standard deviation
X_norm  = (X - repmat(mu,size(X,1),1)) ./  repmat(sigma,size(X,1),1);

</pre>

### 3:debugging ,保证梯度递减正常工作。

要保证gradient descent的过程中，cost function(θ)的值是不断下降并且趋于收敛的，如果不是，那么gradient descent没有正常工作，通常情况都是因为α值选取的不恰当导致的。

### 4:α的选取

不要过大或者过小可以尝试 0.001 - 0.003 - 0.01 - 0.03 - 0. 1 从比较小的值逐渐增加。

### 5: normal equation

公式的推导确实看不懂，可以参考[normal equation的推导](http://dip.sun.ac.za/~hanno/twb264/lesings/lsa.pdf)&[wiki](http://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)#Derivation_of_the_normal_equations)

normal equation适用于很多情况，但如果feature的数量太多为10^6等数量级，计算normalequation的工作量将很大，因为需要计算10^6*10^6的矩阵，所以在feature数量小的时候normal equation比较合适，而feature数量比较大的时候gradient descent比较合适，同时在某些情况下normal equation可能会失效。